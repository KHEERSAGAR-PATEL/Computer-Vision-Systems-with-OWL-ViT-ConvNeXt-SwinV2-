# -*- coding: utf-8 -*-
"""SwinV2_BrainMRI_Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l84_2iWvtRDsBVCCeQSjiUywQsOUwM0x
"""

!pip install torchvision



!pip install torchinfo

!pip install -q git+https://github.com/huggingface/transformers.git

import copy
import cv2
import os
import random

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image

import torch
import torch.nn as nn
from torch import optim
from torch.autograd import Variable
from torch.utils.data import DataLoader, Dataset

from torchinfo import summary

import torchvision
import torchvision.transforms as transforms
import torchvision.utils

from zipfile import ZipFile

from transformers import AutoImageProcessor, Swinv2Model

"""# GPU usage settings"""

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

import os
import glob
import random
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Paths for saving models
CURR_PATH = '/kaggle/working/'
FILE_NAME = os.path.join(CURR_PATH, 'model_seg_pytorch_medical.cnn')
FILE_NAME_PR = os.path.join(CURR_PATH, 'model_segEx_swinV2_medical_21_st.cnn')

# Hyperparameters
r_size = 256
batch_size = 16
ep_num = 20

# Dataset path
DATASET_PATH = '/kaggle/input/mribrain/kaggle_3m'

# Collect all .tif file paths from subdirectories
tif_paths = glob.glob(os.path.join(DATASET_PATH, '**', '*.tif'), recursive=True)

# Build a dictionary: {filename: full_path}
tif_dict = {os.path.basename(p): p for p in tif_paths}

# Define f_images: list of full (image, mask) path pairs
f_images = []
for fname in tif_dict:
    if not fname.endswith('_mask.tif'):
        mask_name = fname[:-4] + '_mask.tif'
        if mask_name in tif_dict:
            f_images.append((tif_dict[fname], tif_dict[mask_name]))

print(f"‚úÖ Total image‚Äìmask pairs (f_images): {len(f_images)}")
if f_images:
    print(f"üñºÔ∏è Sample pair:\n  Image ‚Üí {os.path.basename(f_images[0][0])}\n  Mask  ‚Üí {os.path.basename(f_images[0][1])}")
else:
    print("‚ùå No valid image‚Äìmask pairs found.")

def show_input_sample(f_images):
    if not f_images:
        print("‚ùå No data to show.")
        return

    random.shuffle(f_images)
    img_path, mask_path = f_images[0]

    # Read image using imdecode style
    with open(img_path, 'rb') as f:
        img_data = f.read()
    img = cv2.imdecode(np.frombuffer(img_data, np.uint8), 1)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Read mask using imdecode style
    with open(mask_path, 'rb') as f:
        mask_data = f.read()
    mask = cv2.imdecode(np.frombuffer(mask_data, np.uint8), 1)
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)

    print(f"üîç Image: {os.path.basename(img_path)}")
    print(f"ü©∫ Mask:  {os.path.basename(mask_path)}")
    print(f"Mask stats ‚û§ Max: {mask.max()}, Min: {mask.min()}, Unique: {np.unique(mask)}")

    # Visualize image and grayscale mask
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    axes[0].imshow(img)
    axes[0].set_title("Image (RGB)")
    axes[1].imshow(mask, cmap='gray')  # Ensures proper grayscale rendering
    axes[1].set_title("Mask (Grayscale)")
    for ax in axes:
        ax.axis('off')
    plt.tight_layout()
    plt.show()

show_input_sample(f_images)

image_processor = AutoImageProcessor.from_pretrained("microsoft/swinv2-large-patch4-window12-192-22k")
model_seg = Swinv2Model.from_pretrained("microsoft/swinv2-large-patch4-window12-192-22k").to(device)

"""#Generation batch with pictures-pairs: source image + mask image"""

import cv2
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
import torch
from torchvision import transforms

class SegmentDataset(Dataset):
    def __init__(self,
                 gen_df,
                 transform=transforms.Compose([
                     transforms.ToTensor(),
                     transforms.Resize((r_size, r_size), interpolation=transforms.InterpolationMode.NEAREST_EXACT),
                 ]),
                 mask_color='GRAY'):
        self.gen_df = [item for item in gen_df
                       if "__MACOSX" not in (item if isinstance(item, str) else item[0] and item[1])]
        self.transform = transform
        self.mask_color = mask_color

    def __getitem__(self, index):
        img_name = self.gen_df[index]
        if isinstance(img_name, tuple):  # Handle tuple case for image-mask pairs
            img_name, mask_name = img_name
        else:
            img_name = img_name.strip()
            mask_name = img_name[:-4] + '_mask.tif'

        # Skip __MACOSX
        if "__MACOSX" in img_name or "__MACOSX" in mask_name:
            print(f"Skipping file from __MACOSX directory: {img_name}")
            return self.__getitem__(index + 1 if index + 1 < len(self.gen_df) else 0)

        # --- Load image ---
        img = cv2.imread(img_name, cv2.IMREAD_COLOR)
        if img is None:
            raise IOError(f"Failed to load image: {img_name}")
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # --- Load mask ---
        mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE if self.mask_color == 'GRAY' else cv2.IMREAD_COLOR)
        if mask is None:
            raise IOError(f"Failed to load mask: {mask_name}")

        if self.mask_color == 'GRAY':
            if len(mask.shape) > 2:
                mask = mask[:, :, 0]
        else:
            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)

        # --- Convert to PIL and apply transforms ---
        img_pil = Image.fromarray(img)
        img_in = self.transform(img_pil)

        mask_pil = Image.fromarray(mask)
        mask = self.transform(mask_pil)

        # --- Extract intermediate outputs from model_seg ---
        img_input = image_processor(images=img_pil, return_tensors="pt")

        with torch.no_grad():
            x = model_seg.embeddings(**img_input.to(device))
            input_dimensions = x[1]
            img0 = x[0].squeeze(0)

            x = model_seg.encoder.layers[0](x[0], input_dimensions=input_dimensions)
            img1 = x[0].squeeze(0)

            x = model_seg.encoder.layers[1](x[0], input_dimensions=(input_dimensions[0] // 2, input_dimensions[1] // 2))
            img2 = x[0].squeeze(0)

            x = model_seg.encoder.layers[2](x[0], input_dimensions=(input_dimensions[0] // 4, input_dimensions[1] // 4))
            img3 = x[0].squeeze(0)

            x = model_seg.encoder.layers[3](x[0], input_dimensions=(input_dimensions[0] // 8, input_dimensions[1] // 8))
            x = model_seg.layernorm(x[0])
            img4 = x.squeeze(0)

        return img0, img1, img2, img3, img4, mask, img_in

    def __len__(self):
        return len(self.gen_df)

"""###Network creation (model, loss function, optimizer)"""

class Up_Linear(nn.Module):
    def __init__(self, in_ch, size, coef=1):
        super(Up_Linear, self).__init__()
        self.shuffle = nn.PixelShuffle(upscale_factor=2)

        n_ch = int(coef * in_ch)

        self.ln = nn.Sequential(
            nn.Linear(in_ch * 2, n_ch),
            nn.ReLU(inplace=True),
            nn.Linear(n_ch, in_ch * 2),
            nn.ReLU(inplace=True),
        )

        self.size = size

    def forward(self, x1, x2):
        x = torch.cat((x1, x2), 2)
        x = self.ln(x)
        x = x.permute(0, 2, 1)
        x = torch.reshape(x, (x.shape[0], x.shape[1], self.size, self.size))
        x = self.shuffle(x)
        x = torch.reshape(x, (x.shape[0], x.shape[1], self.size*self.size*4))
        x = x.permute(0, 2, 1)
        return x

class MRI_Seg(nn.Module):
    def __init__(self):
        super(MRI_Seg, self).__init__()

        self.ups3 = Up_Linear(1536, 6, 1)
        self.ups2 = Up_Linear(768, 12, 1)
        self.ups1 = Up_Linear(384, 24, 2)
        self.ups0 = Up_Linear(192, 48, 3)

        self.shuffle = nn.PixelShuffle(upscale_factor=2)

        self.out = nn.Sequential(
            nn.Conv2d(24, 1, kernel_size=1, stride=1),
            nn.Sigmoid()
        )

    def forward(self, x0, x1, x2, x3, x4):
        x = self.ups3(x4, x3)
        x = self.ups2(x, x2)
        x = self.ups1(x, x1)
        x = self.ups0(x, x0)

        x = x.permute(0, 2, 1)
        x = torch.reshape(x, (x.shape[0], x.shape[1], 96, 96))
        x = self.shuffle(x)
        x = transforms.Resize((r_size, r_size))(x)

        x = self.out(x)
        return x

from sklearn.model_selection import train_test_split

# Step 1: Use f_images directly ‚Äì already verified image‚Äìmask pairs
image_mask_pairs = f_images

# Step 2: Train-test split (80% train, 20% test)
train_val_pairs, test_pairs = train_test_split(image_mask_pairs, test_size=0.2, random_state=42)

# Step 3: Further split train into train/val (e.g., 80% train, 20% val)
train_pairs, val_pairs = train_test_split(train_val_pairs, test_size=0.2, random_state=42)

# Step 4: Define transform
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
])

# Step 5: Create datasets (assuming SegmentDataset handles tuple of (img_path, mask_path))
train_dataset = SegmentDataset(gen_df=train_val_pairs, transform=transform)
val_dataset   = SegmentDataset(gen_df=val_pairs, transform=transform)
test_dataset  = SegmentDataset(gen_df=test_pairs, transform=transform)

# Step 6: Create dataloaders
batch_size = 8
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=0)
val_dataloader   = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, num_workers=0)
test_dataloader  = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=0)

net = MRI_Seg().to(device)

criterion = nn.BCELoss()
lr = 0.0001
optimizer = optim.Adam(net.parameters(), lr=lr)

len(train_dataloader)

summary(model=net, input_size=[(1, 2304, 192), (1, 576, 384), (1, 144, 768), (1, 36, 1536), (1, 36, 1536)], col_names=['input_size', 'output_size', 'num_params', 'trainable'])

"""###Non-trained network output"""

net.eval().to(device)

from torch.autograd import Variable
import torch.nn as nn
import torch.optim as optim

# Make sure net, train_dataloader, and device are already defined
ep_num = 8 # or any number of epochs you want
FILE_NAME = 'MRI_segmentation_model.pth'

net = MRI_Seg().to(device)  # No pretrained weights
criterion = nn.BCELoss()
optimizer = optim.Adam(net.parameters(), lr=1e-4)

import os
from tqdm import tqdm  # ‚úÖ Progress bar for epochs

def train_net():
    num_iter = len(train_dataloader)
    ep_init = 0
    best_loss = float('inf')
    best_model_path = os.path.join("/kaggle/working", f"{FILE_NAME}_best.pt")

    # ‚úÖ Wrap epoch loop with tqdm for progress bar
    for epoch in tqdm(range(ep_init, ep_num), desc="Training Progress", unit="epoch"):
        sum_loss = 0

        for data in train_dataloader:
            img0, img1, img2, img3, img4, mask, img_in = data

            optimizer.zero_grad()

            x0 = img0.to(device)
            x1 = img1.to(device)
            x2 = img2.to(device)
            x3 = img3.to(device)
            x4 = img4.to(device)

            output = net(x0, x1, x2, x3, x4)

            loss_bce = criterion(output, mask.to(device))
            loss_bce.backward()
            optimizer.step()

            sum_loss += loss_bce.item()

        avg_loss = sum_loss / num_iter
        print(f"\nüìä Epoch [{epoch + 1}/{ep_num}] | Avg Loss: {avg_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Save best model based on lowest loss
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(net, best_model_path)
            print(f"‚úÖ Best model updated and saved at: {best_model_path}")

    print("\nüéØ Training complete.")
    print(f"üìÅ Best model saved at: {best_model_path}")
    print(f"üîó [Download Link]({best_model_path})")

train_net()

import copy
import cv2
import matplotlib.pyplot as plt
from torch.autograd import Variable

def calc_rect(img_mask):
    ind = np.argwhere(img_mask >= 0.5)
    if len(ind) == 0:
        return None, None
    top_y = min(ind[:, 0])
    bottom_y = max(ind[:, 0])
    top_x = min(ind[:, 1])
    bottom_x = max(ind[:, 1])
    return (top_x, top_y), (bottom_x, bottom_y)

def show_results(i0, i1, i2, i3, i4, y, x1, im_id):
    ii0 = Variable(i0).to(device)
    ii1 = Variable(i1).to(device)
    ii2 = Variable(i2).to(device)
    ii3 = Variable(i3).to(device)
    ii4 = Variable(i4).to(device)

    net.eval()  # Ensure the model is in eval mode
    with torch.no_grad():
        pred = net(ii0, ii1, ii2, ii3, ii4)
    pr = pred[im_id].cpu().numpy()[0]

    xim = copy.deepcopy(x1[im_id].permute(1, 2, 0).cpu().numpy())
    xim = cv2.resize(xim, (r_size, r_size))  # resize if needed

    top_left, bottom_right = calc_rect(pr)
    if top_left is not None:
        cv2.rectangle(xim, top_left, bottom_right, (255, 0, 0), 2)

    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))
    axes[0].imshow(y[im_id].cpu().numpy()[0], cmap='gray')
    axes[1].imshow(pr > 0.5, cmap='gray')
    axes[2].imshow(xim)
    plt.show()

net.eval().to(device)

test_dataloader1 = DataLoader(train_dataset,
                        shuffle=True,
                        num_workers=0,
                        batch_size=1)


test_dataloader2 = DataLoader(test_dataset,
                        shuffle=False,
                        num_workers=0,
                        batch_size=1)

dataiter = iter(test_dataloader1)

i0, i1, i2, i3, i4, y, x1 = next(dataiter)
show_results(i0, i1, i2, i3, i4, y, x1, 0)

dataiter2 = iter(test_dataloader2)

i0, i1, i2, i3, i4, y, x1 = next(dataiter2)
show_results(i0, i1, i2, i3, i4, y, x1, 0)

def calc_accuracy(test_dataloader, set_id, model, sample_num=None):
    batch_size = 1
    if not sample_num:
        N = len(test_dataloader)
    else:
        if sample_num <= 0:
            sample_num = len(test_dataloader)
        N = min(sample_num, len(test_dataloader))

    And = 0
    Uni = 0
    Uni_dice = 0
    IoU_Pic_mean = 0
    calc_mask = 0

    T0 = 0
    T1 = 0
    F0 = 0
    F1 = 0

    for i, data in enumerate(test_dataloader, 0):
        img0, img1, img2, img3, img4, yy, xs = data

        x0 = Variable(img0).to(device)
        x1 = Variable(img1).to(device)
        x2 = Variable(img2).to(device)
        x3 = Variable(img3).to(device)
        x4 = Variable(img4).to(device)

        with torch.no_grad():
            xx1 = model(x0, x1, x2, x3, x4)
        xx1 = xx1[0][0].cpu().detach().numpy()
        yy = yy[0][0].cpu().detach().numpy()
        xx1[xx1 >= 0.5] = 1
        xx1[xx1 < 0.5] = 0

        owl = np.sum(xx1 * yy)
        And += owl
        a_uni_dice = np.sum(xx1 + yy)
        a_uni = a_uni_dice - owl
        Uni += a_uni
        Uni_dice += a_uni_dice

        if a_uni > 0:
            calc_mask += 1
            IoU_Pic = owl / a_uni
            IoU_Pic_mean += IoU_Pic
            if yy.max() == 0:
                F1 += 1
            else:
                if xx1.max() == 0:
                    F0 += 1
                else:
                    T1 += 1
        else:
            T0 += 1

        # Removed: print('{}:  i = {}, And = {}, Uni = {}'.format(set_id, i, And, Uni))

        if i >= N - 1:
            break

    IoU_av = And / Uni
    Dice = 2 * And / Uni_dice
    if calc_mask > 0:
        IoU_Pic_mean = IoU_Pic_mean / calc_mask

    recall = T1 / (T1 + F0 + 1e-8)
    precision = T1 / (T1 + F1 + 1e-8)
    F_measure = (2 * recall * precision) / (recall + precision + 1e-8)

    print(f"\n--- {set_id.upper()} METRICS ---")
    print(f"IoU Average     : {IoU_av:.4f}")
    print(f"IoU Pic Mean    : {IoU_Pic_mean:.4f}")
    print(f"F-measure       : {F_measure:.4f}")
    print(f"Dice Coefficient: {Dice:.4f}")
    print(f"T0 = {T0}, T1 = {T1}, F0 = {F0}, F1 = {F1}\n")

    return IoU_av, IoU_Pic_mean, F_measure, Dice

IoU_tr, IoU_Pic_mean_tr, F_measure_tr, Dice_tr = calc_accuracy(test_dataloader1, 'train', net, sample_num=550)

print('training set: IoU = {}, IoU_pic_mean = {}, F_measure = {}, Dice = {}'.format(IoU_tr, IoU_Pic_mean_tr, F_measure_tr, Dice_tr))

IoU_ts, IoU_Pic_mean_ts, F_measure_ts, Dice_ts = calc_accuracy(test_dataloader2, 'test', net)

print('test set: IoU = {}, IoU_pic_mean = {}, F_measure = {}, Dice = {}'.format(IoU_ts, IoU_Pic_mean_ts, F_measure_ts, Dice_ts))

from IPython.display import display
import ipywidgets as widgets

display(widgets.IntSlider())

